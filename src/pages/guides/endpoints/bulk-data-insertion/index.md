---
title: Bulk Data Insertion API
description: Upload batches of data into Adobe Analytics using the API.
---

# Bulk Data Insertion API

The Bulk Data Insertion API (BDIA) is an Adobe Analytics capability allowing customers to upload server call data in batches of files as opposed to using client-side JavaScript (e.g., tags on web pages or other APIs embedded in application code). The server calls in these batch files can be either current (live) data or historical data. The concept is similar to the [Data Insertion API](/src/pages/1.4/endpoints/data-insertion/index.md) available in the 1.4 API. It provides a productized system that scales, handles errors, and addresses the finer details of inserting data into Adobe Analytics.

Bulk Insertion solves several problems for a variety of use cases. Some use case examples include:

* You want to ingest historical data from a previous analytics system.
* You have an internal analytics collection system that makes it unfeasible to use client-side AppMeasurement. You can use Extract-Transform-Load (ETL) processes to put the data into batch files, then use BDIA to upload them to Adobe Analytics.
* You collect data from devices that have only intermittent connectivity to the internet. These devices store up the interactions until they receive a connection. You can upload the historical data all at once via BDIA.
 
When using BDIA, server calls are sent in batch files. These files are in a specific CSV format where each row of the file defines the details of a server call. Each row, or server call, must specify an identifier for a visitor as well as a timestamp for when the interaction occurred. The server calls must be ordered chronologically by their timestamps, from earliest to latest, in the batch files (this is a requirement of the underlying Analytics system). Each batch file must also be compressed.

## Requirements

The Bulk Data Insertion API has many requirements that your organization must meet. Requirements include file formatting, compression, specific inclusion of certain columns, and hit order.

* Batch files are CSV files that conform to the [RFC-4180 standard](https://tools.ietf.org/html/rfc4180) with one exception: empty lines are ignored.
* Each file consists of a header row (the first row in the file) and subsequent data rows. Rows appear on lines, terminated by a line break (CRLF or LF).
* The last field in a row must not be followed by a comma.
* Rows are fields separated by commas. See the [FAQ](faq.md) on how to pass commas as part of a value.
* Rows must have the same number of columns defined as the header row. Empty fields are allowed by putting two commas or an empty string. For example, `,,` or `,"",`.
* A file must contain ALL of the following columns:
  * `reportSuiteID`
  * `timestamp`
  * `userAgent`
* A file must also contain at least one of the following columns:
  * `marketingCloudVisitorID`
  * `IPAddress`
  * `customerID.[customerIDType].id` with `customerID.[customerIDType].isMCSeed` set to `true`
* A file must also contain at least one of the following columns:
  * `pageURL`
  * `pageName`
  * `pe`
  * `queryString` (NOTE: If only `queryString` is used, at least one of `pageURL`, `pageName`, or `pe` must be set in the `queryString` as a query parameter.)
* All rows in a batch file for any given visitor must be sorted in chronological order by timestamp, from earliest to latest. This requirement ensures that data is ingested in the correct order. Sorting is crucial for proper attribution and analyzing visitor behavior. Adobe does not guarantee the integrity of data processed by BDIA if rows are not arranged in chronological order per visitor.

## Data collection method

The Bulk Data Insertion API allows two methods for data collection. Pick the file format that works best for your organization's workflow.

* **Based on individual columns**: Each column contains a dimension, and each row contains the desired value for that hit.
* **Based on query string**: Place all desired data into the `QueryString` column. This method is ideal when you're working with image requests generated by AppMeasurement or have an existing workflow that generates a query string. Note that the columns listed above are still required.

Most variables have options for either an individual column or a query string parameter. You can use any combination of these two methods. **If a given variable contains data in both an individual column and its query string parameter, the individual column overwrites the query string value.**

See the [Column reference](column-reference.md) for details around header names and query string parameters that you can use for each variable.

## Endpoint URL

Regardless of which data center your report suite resides in, you can direct BDIA calls to a single global host name. However, if you are legally required to have your data processed in a specific part of the world, Adobe offers direct access to regional hosts to ensure your data is processed in the correct location.

* **Global (Auto-routing)**: `POST https://analytics-collection.adobe.io/aa/collect/v1/events`
* **US Processing (Regional)**: `POST https://analytics-collection-va7.adobe.io/aa/collect/v1/events`
* **European Processing (Regional)**: `POST https://analytics-collection-nld2.adobe.io/aa/collect/v1/events`

The following headers are required in the API call:

* **Authorization**: Format is `Bearer <IMS_ACCESS_TOKEN>`. See [Getting started](../../getting-started/index.md) for authentication details.
* **x-adobe-vgid**: Visitor Group ID. A visitor group represents the name of the processing pipeline to use when processing the file. The header value can be any name you choose. Files uploaded to different visitor groups should have disjoint visitor IDs. See [Visitor Groups](visitor-groups.md) for more information.
* **x-api-key**: Client ID issued from the Adobe I/O console. See [Getting started](../../getting-started/index.md) for more information.
* **x-adobe-idempotency-key**: *OPTIONAL* - File ID. Every file ingest transaction receives a GUID to uniquely identify that ingest event. You can use this header to pass in your own identifier with each request. If an API call does not include this header, Adobe automatically generates its own and returns it with the response.

## Response details

With a file ingest POST request, a file object is returned in the response. That file can contain the following fields:

|Field|Datatype|Description|
|--|--|--|
| `file_id` | `string `| Unique identifier for the file upload transaction |
| `visitor_group` | `string` | Name of the visitor group submitted in the `x-adobe-vgid` header field |
| `size` | `long` | Size, in bytes, of the uploaded file |
| `received_date` | `long` | Timestamp when file upload was received in Unix time. |
| `rows` | `int` | The number of rows contained in the file |
| `invalid_rows` | `int` | The number of invalid rows identified in the file |
| `upload_name` | `string` | Name of the file submitted with the request |
| `status` | `string` | Long form of `status_code` |
| `status_code` | `string` | `UPLOADED` or `REJECTED` | 
| `processing_log` | `string` | Notes about any issues found during processing. Up to 10 rows of each error type will be explicitly mentioned, summarized results for more than 10. |
| `idempotency_key` | `string` | If submitted as a header value, then this is the submitted value, else it is the internally generated `file_id` |

## Sample Call

```sh
curl -X POST -H "x-adobe-vgid:prod-18" -H "Authorization: Bearer <IMS_ACCESS_TOKEN>" -H "x-api-key: <CLIENT_ID>" -F file=@/tmp/ingest_file.gz "https://analytics-collection.adobe.io/aa/collect/v1/events"
```
